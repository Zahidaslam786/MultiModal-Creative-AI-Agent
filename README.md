# üé® Multi-Modal Creative AI Agent

A sophisticated AI system designed to bridge the gap between textual descriptions and visual intelligence. This project integrates **Image Synthesis** and **Visual Perception** into a single cohesive agentic workflow.

## üöÄ Overview
This agent functions as both a creator and an observer. It can synthesize high-fidelity images from complex prompts and then "see" and analyze those images to provide structured descriptive feedback. This demonstrates the power of multi-modal AI in creative and analytical pipelines.

## üõ†Ô∏è The Workflow (Step-by-Step)
1. **Latent Diffusion Synthesis**: The agent utilizes the **Stable Diffusion** pipeline (Text Encoder, UNet, and VAE) to generate high-quality images from text descriptions.
2. **Visual Perception & Analysis**: Using **Vision-Language Models** (like BLIP), the agent interprets the generated output, autonomously describing objects, styles, and artistic composition.
3. **Hardware & Inference Optimization**: The system is specifically optimized to run on **T4 GPUs** using `float16` precision and the `accelerate` library for high-speed, memory-efficient inference.
4. **End-to-End Pipeline**: A unified Python interface that handles the transition from text-to-image and image-to-text without manual intervention.

## üîß Technical Stack
- **Frameworks**: Hugging Face Diffusers & Transformers.
- **Core Library**: PyTorch.
- **Hardware Optimization**: Accelerate for efficient GPU memory management.
- **Modality**: Multi-modal (Text + Vision).

## üìä Visual Gallery (Agent Outputs)
The following gallery showcases images generated by the pipeline along with the agent's autonomous vision analysis.

<div align="center">
  <img src="https://github.com/user-attachments/assets/c3341e42-29e0-4ab6-bb44-efc6b9c4e381" width="48%" />
  <img src="https://github.com/user-attachments/assets/b6ad859d-ae58-4b67-8c02-92ea00e04b49" width="48%" />
</div>
